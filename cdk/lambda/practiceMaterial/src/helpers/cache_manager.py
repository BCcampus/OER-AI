"""
Response cache manager for practice material generation.

Provides in-memory caching with LRU eviction and TTL expiration
to reduce LLM API calls for repeated requests.
"""

import time
import hashlib
import logging
from typing import Any, Dict

logger = logging.getLogger(__name__)

# Response cache - stores generated practice materials
# Key: hash of (textbook_id, topic, material_type, difficulty, num_items, num_options/card_type)
# Value: {"result": <generated_content>, "sources": <sources_used>, "timestamp": <unix_time>}
_response_cache: Dict[str, Dict[str, Any]] = {}
CACHE_MAX_SIZE = 100  # Maximum number of cached responses
CACHE_TTL_SECONDS = 3600  # Cache entries expire after 1 hour


def generate_cache_key(
    textbook_id: str,
    topic: str,
    material_type: str,
    difficulty: str,
    num_items: int,
    extra_params: str = ""
) -> str:
    """
    Generate a consistent cache key for practice material requests.
    
    Args:
        textbook_id: UUID of the textbook
        topic: User-provided topic
        material_type: Type of material ('mcq', 'flashcard', 'short_answer')
        difficulty: Difficulty level
        num_items: Number of questions/cards
        extra_params: Additional parameters (e.g., num_options, card_type)
    
    Returns:
        MD5 hash of the request parameters
    """
    cache_string = f"{textbook_id}:{topic}:{material_type}:{difficulty}:{num_items}:{extra_params}"
    return hashlib.md5(cache_string.encode()).hexdigest()


def get_cached_response(cache_key: str) -> Dict[str, Any] | None:
    """
    Retrieve cached response if available and not expired.
    
    Args:
        cache_key: Cache key generated by generate_cache_key
    
    Returns:
        Cached response dict or None if not found/expired
    """
    global _response_cache
    
    if cache_key not in _response_cache:
        return None
    
    cached_entry = _response_cache[cache_key]
    timestamp = cached_entry.get("timestamp", 0)
    
    # Check if cache entry is expired
    if time.time() - timestamp > CACHE_TTL_SECONDS:
        logger.info(f"Cache entry expired for key: {cache_key}")
        del _response_cache[cache_key]
        return None
    
    logger.info(f"Cache HIT for key: {cache_key}")
    return cached_entry


def set_cached_response(
    cache_key: str,
    result: Dict[str, Any],
    sources: list[str]
) -> None:
    """
    Store response in cache with LRU eviction if cache is full.
    
    Args:
        cache_key: Cache key generated by generate_cache_key
        result: Generated practice material result
        sources: List of source citations
    """
    global _response_cache
    
    # Implement simple LRU: if cache is full, remove oldest entry
    if len(_response_cache) >= CACHE_MAX_SIZE:
        # Find and remove the oldest entry
        oldest_key = min(_response_cache.keys(), key=lambda k: _response_cache[k].get("timestamp", 0))
        logger.info(f"Cache full, evicting oldest entry: {oldest_key}")
        del _response_cache[oldest_key]
    
    _response_cache[cache_key] = {
        "result": result,
        "sources": sources,
        "timestamp": time.time()
    }
    logger.info(f"Cache SET for key: {cache_key} (cache size: {len(_response_cache)})")


def get_cache_stats() -> Dict[str, Any]:
    """
    Get current cache statistics.
    
    Returns:
        Dictionary with cache size, max size, and TTL
    """
    return {
        "size": len(_response_cache),
        "max_size": CACHE_MAX_SIZE,
        "ttl_seconds": CACHE_TTL_SECONDS
    }


def clear_cache() -> None:
    """
    Clear all cached responses.
    Useful for testing or manual cache invalidation.
    """
    global _response_cache
    _response_cache.clear()
    logger.info("Cache cleared")
